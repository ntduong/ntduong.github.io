<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Random Notes - ML</title><link href="/" rel="alternate"></link><link href="/feeds/ml.atom.xml" rel="self"></link><id>/</id><updated>2021-01-18T00:00:00+09:00</updated><entry><title>My 2021 Machine Learning's resolution</title><link href="/my-2021-machine-learnings-resolution.html" rel="alternate"></link><published>2021-01-18T00:00:00+09:00</published><updated>2021-01-18T00:00:00+09:00</updated><author><name>Duong Nguyen</name></author><id>tag:None,2021-01-18:/my-2021-machine-learnings-resolution.html</id><summary type="html">&lt;h1 id="2021-ml-hopeful-interests"&gt;2021 ML hopeful &amp;amp; interests&lt;/h1&gt;
&lt;p&gt;I'm seriously interested in &lt;strong&gt;practical&lt;/strong&gt;, &lt;strong&gt;inclusive&lt;/strong&gt; ML:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Practical: how to apply ML techniques to daily problems&lt;/li&gt;
&lt;li&gt;Recently, I was amazed learning about Japanese farmers use computer vision tools to remotely analyze images (taken by drones) of cabbage fields and extract a lot of detailed information about â€¦&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1 id="2021-ml-hopeful-interests"&gt;2021 ML hopeful &amp;amp; interests&lt;/h1&gt;
&lt;p&gt;I'm seriously interested in &lt;strong&gt;practical&lt;/strong&gt;, &lt;strong&gt;inclusive&lt;/strong&gt; ML:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Practical: how to apply ML techniques to daily problems&lt;/li&gt;
&lt;li&gt;Recently, I was amazed learning about Japanese farmers use computer vision tools to remotely analyze images (taken by drones) of cabbage fields and extract a lot of detailed information about the cabbages (size, shape, ready for harvest, etc.) all from individual images. I think we need more applications like this; not only in agriculture but also in various fields&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I don't discount other non-ML tools as less effective and ML is certainly &lt;strong&gt;NOT&lt;/strong&gt; a magical cure-all. Stay clear from the hype and be rational though.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inclusive and resource-effective ML: How to conduct ML research and use ML techniques with low-powered/limited computational resource&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Revisit existing techniques with new insights and make them more resource-effective. I personally find research work along this line very interesting and valuable. This &lt;a href="https://arxiv.org/abs/2011.14826"&gt;revisiting Rainbow paper&lt;/a&gt; is an example.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A lot of deep learning advance (e.g., in computer vision and language models recently) can be attributed to huge computing resources and powerful hardware (accelerators such as GPU, TPU). Scaling ML with more resources is certainly a valid direction, but I believe the other direction is super important. Many people couldn't afford computing resource to conduct ML research or apply ML to their own problems. How could the ML community come together to solve this problem?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After a personal 2020 ML winter, my ML resolution this year as an aspiring ML practitioner, inlined with the renewed interests above are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Establish a small set of basic yet well-understood tools (e.g., algorithms, datasets, libraries), which can be readily used as starters for practical problems&lt;/li&gt;
&lt;li&gt;Focus on experimenting/trying things out quickly (vs. pure paper reading)&lt;/li&gt;
&lt;li&gt;Gain deep insights in the basics, explore and follow resource-effective ML direction&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="reflect-on-past-ml-adventure-and-resolution"&gt;Reflect on past ML adventure and resolution&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Feel free to skip the below rambling. It is a funny self-reflection on my past ML experience.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="2012-2014"&gt;2012-2014&lt;/h2&gt;
&lt;p&gt;My interest in Machine Learning (ML) has been on and off over the past years. I came into contact with ML back in 2012 working on &lt;a href="https://search.ieice.org/bin/summary.php?id=e97-d_7_1822"&gt;probability density difference estimation&lt;/a&gt; for my undergrad thesis. While this work doesn't have much to be proud of, it succeeded in whetting my appetize for ML as a multi-disciplinary field in which science and engineering come together to do cool things. With slight inclination for applied math and computer programming, I was excited to start my ML journey. Came my next 2-year master course when I spent time tackling problems with data distribution discrepancy between training and testing time in supervised learning. Through course work and research seminars, I got to learn about ML techniques (but deep learning wasn't popular back then at least in my surrounding), and work through excellent textbooks, such as the popular &lt;a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/"&gt;C. Bishop's PRML&lt;/a&gt; or &lt;a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/"&gt;Hastie+etal's The Elements of Statistical Learning&lt;/a&gt;. Even now when deep learning has become the mainstream, I'd still recommend these books for people coming to the field.&lt;/p&gt;
&lt;h2 id="2015"&gt;2015&lt;/h2&gt;
&lt;p&gt;Unfortunately, 2015 was the year when I lost interest in my fundamental research. I got the degree and managed to get &lt;a href="http://proceedings.mlr.press/v45/Nguyen15.html"&gt;my thesis paper&lt;/a&gt; accepted at conference. But I wasn't happy with the state of my research; struggling to find the motivation to further my research. Was it a dead-end? Who told you research is always fun and joyful and rewarding...In short, I struggled to see the meaning of my research, I didn't see how I could apply my research to the real life. I couldn't lie in the research grant proposal about the usefulness and practicality of my research topic. Then the realistic I decided to &lt;strong&gt;pause&lt;/strong&gt; my research journey and instead took a software development job. As a software developer, the main job is to build things; and it's mostly fulfilling to see the real-life impact (be it useful or useless) on users.&lt;/p&gt;
&lt;h2 id="2016-2018"&gt;2016-2018&lt;/h2&gt;
&lt;p&gt;On to 2016-2018, my ML interest was still alive and kicking; not enough to convince me to come back to research fulltime, but enough to have me resume paper reading and following research commnunities. I started feeling joy and hopeful again :). Deep learning has then become the mainstream and the whole ML field has become too big with astronomical number of papers and topics to follow along. It was exciting ML time but depressing and stressful to find a way back, at least for me. Lots of things I knew/assumed from the past experience have become obsolete or no longer applicable so re-learn is unavoidable! I decided to learn about new areas/topics that I didn't touch on in the past, including Reinforcement Learning (RL) and deep learning architectures in computer vision and NLP tasks (such as Resnet, Transformer). I came to realize with very limited personal time (shared with family commitment), I could hardly go deep and mostly stop at superficial level. But it was okay during this &lt;em&gt;topic-sampling&lt;/em&gt; period. &lt;strong&gt;Exploration is always worthwhile during early learning!&lt;/strong&gt;. This year, I gave some techtalks at a local ML meetups on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RL's policy gradient methods (&lt;a href="https://ntduong.github.io/policy-gradient-rl/#/"&gt;slides&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;TCAV technique for ML interpretability (&lt;a href="https://ntduong.github.io/tcav-fest-18/"&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="2019"&gt;2019&lt;/h2&gt;
&lt;p&gt;2019 was my RL year when I spent most of my commute time reading the &lt;a href="http://incompleteideas.net/book/the-book.html"&gt;Sutton &amp;amp; Barto's textbook&lt;/a&gt; along with various RL papers; and also implemented algorithms at weekends. I was lucky having a chance to work (in my &amp;lt; 20% time) with research colleagues at the end of 2019, on attention agent in vision RL tasks. The work resulted in a &lt;a href="https://arxiv.org/abs/2003.08165"&gt;GECCO 2020 best paper award&lt;/a&gt;. 99%+ credit go to my colleagues, as I only contributed a tiny part, but the whole experience was rewarding.&lt;/p&gt;
&lt;h2 id="2020"&gt;2020&lt;/h2&gt;
&lt;p&gt;Then came my 2020 ML winter when I lost interest and mostly stopped reading research papers (partially because I didn't commute due to Covid-19 :().&lt;/p&gt;
&lt;h2 id="2021"&gt;2021+&lt;/h2&gt;
&lt;p&gt;Here comes 2021, I'm reviving my ML love with hopefully more realistic and focused targets. And my ML love will need to wrestle with my daughter for my dedication. I hope to find a good balance!&lt;/p&gt;
&lt;p&gt;Until then 2022!&lt;/p&gt;</content><category term="ML"></category><category term="ML"></category><category term="Opinion"></category></entry></feed>