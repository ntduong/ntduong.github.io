<!DOCTYPE html>
<html lang="en">
<head>
          <title>Random Notes - An opinionated case against A/B testing: Exploration-vs-Exploitation</title>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Random Notes Full Atom Feed" />
        <link href="/feeds/decision-making.atom.xml" type="application/atom+xml" rel="alternate" title="Random Notes Categories Atom Feed" />




    <meta name="tags" content="experiment" />
    <meta name="tags" content="A/B test" />
    <meta name="tags" content="multi-armed bandits" />
    <meta name="tags" content="exploration" />
    <meta name="tags" content="exploitation" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">Random Notes</a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li><a href="/pages/about-me.html">About me</a></li>
            <li><a href="/category/algorithm.html">Algorithm</a></li>
            <li><a href="/category/data-structures.html">Data structures</a></li>
            <li class="active"><a href="/category/decision-making.html">Decision Making</a></li>
            <li><a href="/category/stats.html">Stats</a></li>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="/an-opinionated-case-against-ab-testing-exploration-vs-exploitation.html" rel="bookmark"
         title="Permalink to An opinionated case against A/B testing: Exploration-vs-Exploitation">An opinionated case against A/B testing: Exploration-vs-Exploitation</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2021-02-14T00:00:00+09:00">
      Sun 14 February 2021
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="/author/duong-nguyen.html">Duong Nguyen</a>
    </address>
    <div class="category">
        Category: <a href="/category/decision-making.html">Decision Making</a>
    </div>
    <div class="tags">
        Tags:
            <a href="/tag/experiment.html">experiment</a>
            <a href="/tag/ab-test.html">A/B test</a>
            <a href="/tag/multi-armed-bandits.html">multi-armed bandits</a>
            <a href="/tag/exploration.html">exploration</a>
            <a href="/tag/exploitation.html">exploitation</a>
    </div>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <h1 id="motivation">Motivation</h1>
<p>Nowadays, a lot of companies (esp. tech companies) have been embracing data-driven decision making practice. Almost every business decision such as <em>what background color should be used for the company website</em> or <em>whether a new kitty photo filter should be released</em> requires justification with some experimental perf metrics. A/B testing has become a de facto tool for trying out and <strong>comparing</strong> multiple ideas/solutions (aka, <em>arms</em>, <em>options</em>) simultaneously, often <strong>on production traffic</strong>, such as user requests. In business, A/B testing is a general term referring to the practice of conducting randomized controlled trials (aka, <em>experiment</em>) on real/production users. </p>
<h2 id="a-simple-ab-test">A simple A/B test</h2>
<p>Let's consider an oversimplified A/B test on a company homepage background color, in which we want to decice which color, <em>red</em> or <em>blue</em> should be used. Say the CEO agrees to give me 20% of the website traffic to experiment on for 1 month. I then get a fair coin with 50% chance of turning up head/tail. Whenever a user request comes, I flip the coin, if it's head then the user gets to see <em>red</em>; otherwise the user gets to see <em>blue</em>. This process applies up to 20% overall traffic, which divides up more or less equally into the red group and the blue group (as I use a fair coin). I also need to decide key performance metrics to be measured on experimental traffic. Let's pretend for a user comes to the website, I want to measure the duration they stay on the site (in minutes) as a proxy for user engagement/happiness with the background color; the longer users stay, the better. After 1 month of experiment, I compare the average happiness scores of the red and blue groups and turns out the red group has the higher score. I draw conclusion that red is better and propose red color to the CEO (final decision maker). That's it, a simple yet typical example of an A/B test.</p>
<h2 id="more-boring-details">More (boring) details</h2>
<p>A typical A/B test usually includes a single baseline (aka <strong>Control group</strong>) and 1 or more experimental arms (aka <strong>Treatment groups</strong>).</p>
<ul>
<li>Treatment arms and Control represent different options we'd like to try out and compare; but other than that they are under kept similar (hence, <em>controlled</em>). A treatment arm is usually compared against the Control and/or other treatment arms, in terms of key performance metrics.</li>
<li>What metrics should be used depend on specific business problems, such as click-through-rate (CTR) on ads banner, user visit rate on website etc.</li>
<li>How much traffic should be used: usually companies can't afford running experiments on large production traffic (except when they are inevitably neccessary); lest a significant part of users will be treated weird/sub-optimal experience (ahem, it's an experimental feature anyway!). It's crucial to have a <strong>representative</strong> traffic for experiment; since we want to draw business insights/conclusion from the experimental traffic and extrapolate to full production user traffic.</li>
<li>How long should a A/B test run? It again depends on specific problems and domains, but short to medium-term (up to 1 month) experiments are very common.</li>
<li>Experiment sizing (how much traffic, how long to run) is critical to the success of a test. The sizing needs to be done carefully with statistical rigor before starting the test. Once it's decided, it should be kept until the end; modifying these midway essentially invalidate the experiment results (e.g., some p-hacking tricks like extending test duration or increase traffic midway are unacceptable!)</li>
</ul>
<p>A/B testing isn't new or exotic tool at all, in fact a lot of A/B tests (aka, <strong>experiment</strong>) have been conducted daily by e.g., scientists, for drug testing, discovering new materials, chemical substances, new vaccinces, testing new medical treatments etc. With all due respect to the rigorous science of A/B testing, I believe it's pretty solid and reasonable tool for decision making. </p>
<p>In this post, however <strong>I want to make a case agains A/B testing, that it is NOT a silver bullet, and may not work well in many practical scenarios</strong>.</p>
<h1 id="when-ab-testing-fails">When A/B testing fails</h1>
<p>Let's revisit the <em>simple</em> red-or-blue example above. You may notice some naivety in my conclusion that red is better color. With just 1 month of data, how dare are you to conclude that red is better than blue? Turns out the experiment was running in the Christmas season and people seemed to prefer red (who doesn't like red-and-white Santa Claus with big presents :)), and somehow people seeing red stayed on the website longer, and tilted my happiness metric into red's favor. Sadly, soon after the Christmas season was over, the traffic to the red homepage went down drastically and many users voiced their concern/disastifaction with the red color! </p>
<p>So you see the first flaw with short/mid-term A/B test: <em>some counfounding factors (out of experimenter's control?) can corrupt the performance metrics and lead to wrong conclusion, in hindsight</em>. </p>
<p>As a diehard A/B tester, I argue what if I was allowed to run the experiment for longer, say a full year, I'd surely figure out the best color. No way I can get approval for that given the clear risk of losing out users. But even if I can run the experiment for that long, I may end up observing no significant difference between red and blue in key metrics. How come? Turns out <strong>in hindsight again</strong>, people prefer red during winter and fall; and blue during autumn and summer. Another possible scenario is user base shift over time, when the experiment starts, more users prefer red over blue, but somehow during the year, more blue users come and red users leave. <em>So averaging over the whole year, blue and red are comparable, each has its own good and bad periods. Just by observing the results in the end, no conclusion can be draw on what color is better</em>.</p>
<h2 id="the-key-assumption-of-ab-testing">The key assumption of A/B testing</h2>
<blockquote>
<blockquote>
<p>The key underlying assumption of A/B testing is that there is a single winner, applicable to all user traffic over long time.</p>
</blockquote>
</blockquote>
<p>When this assumption holds, A/B testing works really well and help us figure out the clear winner quickly. But how often does this assumption hold true in practice? Many real-world interesting/critical problems don't have a single clear winner solution for all users, not to mention over long time. It's arguably more often that we're in the situation when different user groups prefer different solutions and working solutions shift over time. </p>
<blockquote>
<blockquote>
<p>In other words, multiple solutions should co-exist and serve their own user base over time!</p>
</blockquote>
</blockquote>
<p>Even when A/B testing works well (and there exists a single clear winner), it still has an critical drawback; that is <strong>during experimental phase, a significant part of users are exposed to sub-optimal solutions</strong>; since A/B testing usually divides up traffic into equal groups, and only one group gets the winner treatment.</p>
<blockquote>
<blockquote>
<p>This situation incurs <strong>regret</strong>, loss of opportunity for the experiment runner (failed to deliver best experience to users) and sub-optimial experience hurts users and may turn them away from the business.</p>
</blockquote>
</blockquote>
<p>So what are alternatives to A/B testing when it failed? Actually to be fair, the problem with A/B testing is more about how we interpret and make decision based on the experiment results; not the experiment itself (experimentation is good!). It's often that after a short period of exploring multiple options equally, a single option is selected to be the winner and then to be deployed to all traffic.</p>
<blockquote>
<blockquote>
<p>It's <strong>explore-then-exploit</strong> (aka, explore-then-commit) strategy, nothing more!</p>
</blockquote>
</blockquote>
<p>The problem is that we commit (too soon) to a single solution over long term for all users; which is usually sub-optimal. Thus, a natural alternative is to keep all options <strong>proportionally to how good individual options are</strong>, i.e., keep good balance between exploration and exploitation!</p>
<h1 id="explore-vs-exploit">Explore vs Exploit</h1>
<p>The exploration-vs-exploitation tradeoff is at the core of sequential decision making. In order to perform well over a long run, it's critical to juggle exploration and exploitation effectively. We need a flexible strategy which can adapt to the dynamics of the environment in which the strategy operates on. Too abstract? Let's revisit the red-or-blue problem, instead of committing to red after the experiment, what if we keep serving both red and blue; but with <strong>a catch</strong>: the corresponding traffic should be proportional to how good a color currently performs, i.e., when red is doing better (based on user happiness metric), we should allocate more traffic to red than blue; and vice versa (when blue is doing better). <strong>The key here is the traffic for each option is scaled dynamically over time according to user preference</strong>; the better a color performs, the higher traffic it receives. This sounds too good to be true; how can we do this? One popular approach to situations involving explore-vs-exploit across multiple options is the so-called <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"><strong>multi-armed bandits</strong></a>, which is a family of algorithms for decision making with strong theoretical support and empirical successes. Multi-armed bandits are a key ingredient in a lot of successful real-world applications, such as Netflix art work recommendation, website optimization, ads optimization. A detailed treatment of multi-armed bandits algorithms is obviously out of scope of this post; please refer to the References section for good resource on the topic.</p>
<blockquote>
<blockquote>
<p>The key point to remember here is that multi-armed bandits allow for continuously exploring multiple options while exploiting best performers at a time. The exploration is proportional to individual options' performance.</p>
</blockquote>
</blockquote>
<p>Astute readers may wonder how to incorporate contextual information, such as user profile into the decision making here in addition to solely relying on performance metrics. Yes, <strong>contextual bandits</strong> is an algorithm in the multi-armed bandits family, taking into account contextual information when determining which option to take at a time for a particular context. For example, for a user with strong preference and watch history of anime, the algorithm should probably explore anime for this user, even though anime genre doesn't perform as well as other genres (like romantic drama) at the time.</p>
<p>So should we abandon A/B testing for multi-armed bandits? It likely depends on your specific problems.</p>
<ul>
<li>A/B testing enables decision making based on short-term experiments; with <strong>explore equally-then-commit</strong> strategy</li>
<li>OTOH, multi-armed bandits is always-on, continuously running experiment, in which multiple options are explored proportionally and best options at a time are exploited accordingly; essentionally <strong>explore-vs-exploit</strong> juggling strategy. It sounds like a great strategy but comes with a cost for the balancing act. Devising good explore-vs-exploit balancing strategy in general is still an active research problem</li>
</ul>
<h1 id="middle-ground-proposal">Middle ground Proposal</h1>
<p>Despite its limitation, A/B testing remains a widely adopted practice thanks to its working well at relatively low cost (vs. multi-armed bandits) in practice. To end this long post, I want to propose some middle ground approach, taking into account the explore-vs-exploit trade-off.</p>
<h2 id="proposal-1-continuously-experiment-and-reevaluate-the-status-quo-to-search-for-the-better">Proposal 1: Continuously experiment and reevaluate the status-quo to search for the better</h2>
<p>Committing to a single winner in short-term is reasonable; it's like accepting your local optimum with low cost. However, we should avoid sticking to a single winner for too long. Instead, a good strategy is to continuously run short A/B tests to reevaluate the status-quo and promote new better options when appropriate.</p>
<h2 id="proposal-2-incorporate-simple-exploration-strategy-in-addition-to-exploiting-the-current-best-option">Proposal 2: Incorporate simple exploration strategy in addition to exploiting the current best option</h2>
<p>Consider adopting simple exploration strategy from multi-armed bandits literature:</p>
<ul>
<li><span class="math">\(\epsilon\)</span>-greedy: While commiting to the current best solution most of the time; we should set aside a small budget for random exploration, say for 5% of the time serving a <strong>not-the-current-best</strong> solution</li>
<li>Upper confidence bound (UCB): Explore an option proportionally to how much we know about its performance. Intuitively, the less we know about an option, the higher potential value it can bring (to our surprise); so we should try it more. Reversely, as we know more about an option, we may reduce its frequency. <strong>Essentially, explore to reduce the entropy!</strong></li>
</ul>
<h1 id="references">References</h1>
<ol>
<li><a href="https://www.oreilly.com/library/view/bandit-algorithms-for/9781449341565/">Bandits Algorithms for Website Optimization</a>: a old yet good enough introductory book on basic bandits algorithms.</li>
<li><a href="https://www.cambridge.org/core/books/bandit-algorithms/8E39FD004E6CE036680F90DD0C6F09FC">Bandits Algorithms</a>: an excellent book with rigorous theoretical treatment of bandits algorithms by Tor Lattimore and Csaba Szepesvári.</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>